---
title: "LBB_RM_Draft"
author: "Amalia Purieka"
date: "1/12/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)x
```

# Background

Given a data related to house sales, we are making a regression model. We are going to predict ('Prices') based on several given predictors. The data is taken from online open resources.

# Library

We load the necessary library.

```{r}
library(tidyverse)
library(caret)
library(plotly)
library(data.table)
library(GGally)
library(tidymodels)
library(car)
library(scales)
library(lmtest)

options(scipen = 100, max.print = 1e+06)
```


# Data

Our data is stored inside the 'data_input' folder with HousePrices_HalfMil.csv name.

```{r}
dhp <- read.csv("data_input/HousePrices_HalfMil.csv")
dhp
```

## Data Inspection

Check structure data using str()

```{r}
str(dhp)
```

Our data has 500000 rows with 16 variables.
Check the numeric variables correlation using ggcorr()

```{r}
ggcorr(dhp, label = T)
```
From the above heatmap we can observe which variable that significantly affect the 'Prices'

## Data Preprocessing

Check missing data.

```{r}
# your code here
colSums(is.na(dhp))
```

Our data has no missing value.

Then proceed to transform column based on the observation result from the structure data.

```{r}
# 
dhp <- dhp %>% 
  mutate(Garage = as.factor(Garage),
         FirePlace = as.factor(FirePlace),
         Baths = as.factor(Baths),
         White.Marble = as.factor(White.Marble),
         Black.Marble = as.factor(Black.Marble),
         Indian.Marble = as.factor(Indian.Marble),
         Floors = as.factor(Floors),
         City = as.factor(City),
         Solar = as.factor(Solar),
         Electric = as.factor(Electric),
         Fiber = as.factor(Fiber),
         Glass.Doors = as.factor(Glass.Doors),
         Swiming.Pool = as.factor(Swiming.Pool),
         Garden = as.factor(Garden)
         )
```


## Cross Validation

Splitting the data so that we could use this data as data_train and data_test as well.

```{r}
# Mengambil data secara acak
set.seed(123)
row_hp <- nrow(dhp)
indexhp <- sample(row_hp, row_hp*0.8)

hp_train <- dhp[ indexhp, ]
hp_test <- dhp[ -indexhp, ]
```

# Exploratory Data Analysis

Observe data in each column using table()

```{r}
table(dhp$Garage)
table(dhp$FirePlace)
table(dhp$Baths)
table(dhp$White.Marble)
table(dhp$Black.Marble)
table(dhp$Indian.Marble)
table(dhp$Floors)
table(dhp$City)
table(dhp$Solar)
table(dhp$Electric)
table(dhp$Fiber)
table(dhp$Glass.Doors)
table(dhp$Swimming.Pool)
table(dhp$Garden)
```

# Modeling and Model Interpretation

Develop a regression linear model with all the provided predictor variables and another regression linear model with selected predictor variables to predict the 'Prices'. Here we are using stepwise as feature selection method.

```{r}
# modeling
model_hp <- lm(Prices ~ ., hp_train)

summary(model_hp)

# stepwise
modelstephp <- step(model_hp, trace = 0)

summary(modelstephp)
```

Interpretation:

- modelstephp is using 'Area', 'Garage', 'Fire_Place', and 'Baths' as predictor variable to predict 'White_Marble', 'Black_Marble', 'Floors', 'City', 'Solar', 'Electric', 'Fiber', and 'Glass_Doors'
- by removing the other variables, both of the Multiple R-Squared and Adjusted R-Squared are not significantly affected, thus we will go with the modelstepdc instead


# Prediction and Model Evaluation

## Prediction

Predict the 'Prices' of modelstephp with the 'hp_test' data, then count the error value to check the model performance.
We are going to use below matrix:
- MAE
- RMSE

Predict the model and use MAE - RMSE to evaluate the model

```{r}
# predict selling price of new data
pred_hp <- predict(modelstephp, newdata = hp_test)

# MAE
MAE(pred_hp, hp_test$Prices)

# RMSE
RMSE(pred_hp, hp_test$Prices)
```

From the output above, our model gives small value of the result of the predicted deviations on average. We are also using RMSE to give a more sensitive error reading.

## Assumption Checking

Check below assumption:

- Normality

Determination

- p-value> 0.05: Do not reject $ H_0 $ or residual normally distributed
- p-value <0.05: Reject $ H_0 $ or the residual is not normally distributed

- Homoscesdasticity

Determination

- p-value> 0.05: Does not reject $ H_0 $ or error is constant (Homocesdasticity)
- p-value <0.05: Reject $ H_0 $ or error is not constant (Heteroscesdasticity)

- Multicolinearity

When the VIF value is more than 10 it means multicollinearity occurs. If this happens, you can select one of the variables that is removed from the model that has VIF> 10.

Assumption:

- There is a correlation between the target variable and predictor
- There is no correlation between the predictors and other predictors


```{r}
# Normality
library(nortest)
ad.test(modelstephp$residuals)
```

```{r}
plot(density(modelstephp$residuals))
```
Our data has normal distribution.

```{r}
# Homoscesasticity
bptest(modelstephp)
```
Our data has not fulfil this assumption (p-value more than 0.05), we need to improve the model later.

```{r}
data.frame(prediction = modelstephp$fitted.values,
           residual = modelstephp$residuals) %>% 
  ggplot(aes(prediction, residual)) +
  geom_hline(yintercept = 0) +
  geom_point( color = "skyblue4") +
  theme_minimal()
```

```{r}
# Multicolinearity
vif(modelstephp)
```
Our data has no correlation between its predictor.

# Model Improvement

Transform  the targetvariable using sqrt() then build a new model.
```{r}
# Merubah target variabel
dhp_new <- dhp %>% 
  mutate(Prices = sqrt(Prices))

# modeling
model_hp_new <- lm(Prices ~ ., dhp_new)

# stepwise
modelstephp_new <- step(model_hp_new, trace = 0)

# Homocesdasticity
bptest(modelstephp_new)
```

p-value less than 0.05.
Now our model has fulfil all the assumptions.

# Conclusion

Our model has passed the evaluation and fulfil all the assumption. It is using variables: 'Area', 'Garage', 'Fire_Place', and 'Baths' as predictor variable to predict 'White_Marble', 'Black_Marble', 'Floors', 'City', 'Solar', 'Electric', 'Fiber', and 'Glass_Doors'. Adjusted R-Squared is 1 which means our model has well representated the whole data.

# Reference

https://www.kaggle.com/greenwing1985/housepricing